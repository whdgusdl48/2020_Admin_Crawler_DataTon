# -*- coding: utf-8 -*-
"""word_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ugJ-O5KhO3JkRbMerCMVi10si722MxKx
"""

import re
import pandas as pd

from collections import Counter

from konlpy.tag import Hannanum
from krwordrank.word import KRWordRank
from nltk import Text
from nltk.tokenize import regexp_tokenize


def remove_special_characters(texts):
    texts = re.sub("[-+=,#/\?:^$!@%&()~]", "", texts)
    return texts


def word_analysis(str):

    en = regexp_tokenize(str, "([A-Za-z]+),https")
    T = list(Text(en))
    texts = remove_special_characters(str)
    noun = Hannanum().nouns(texts)
    noun.extend(T)
    print(noun)
    count = Counter(noun)
    noun_list = count.most_common(30)
    print(count)
    index = 0
    count_ = []
    for v in noun_list:
        if len(v[0]) >= 1:
            index = index + 1
            count_.append(v)
            if index >= 10:
                break

    return count_

def make_data_set(ds_):#csv파일 넣기
    # ds=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/result.tsv', delimiter='\t')
    ds = ds_
    data = ds.iloc[:, 3]

    for i in range(len(data)):
        texts = data[i]
        word_list = word_analysis(texts)  # 텍스트 분석해서 키워드 추출
        text=""
        for j in word_list:
          text=text+j[0]+","
        print(text)

        ds.loc[i, 'words'] = text

    ds.to_csv('/content/drive/MyDrive/Colab Notebooks/result_word.tsv', index=False, sep="\t")

ds=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/result.tsv', delimiter='\t')
make_data_set(ds)