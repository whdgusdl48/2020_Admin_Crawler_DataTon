{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBeuDfpUHo7p"
      },
      "source": [
        "from konlpy.tag import Hannanum\r\n",
        "from collections import Counter\r\n",
        "import re\r\n",
        "from nltk import Text\r\n",
        "from nltk.tokenize import regexp_tokenize\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xay1wEXIAqX1"
      },
      "source": [
        "def remove_special_characters(texts):\r\n",
        "  texts=re.sub(\r\n",
        "          '[-+=,#/\\?:^$!@%&()~]','',texts\r\n",
        "      )\r\n",
        "  return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNcuSo-YgOeT"
      },
      "source": [
        "def word_analysis(str):\r\n",
        "    en = regexp_tokenize(str, \"([A-Za-z]+),https\")\r\n",
        "    T=list(Text(en))\r\n",
        "    texts = remove_special_characters(str)\r\n",
        "    noun = Hannanum().nouns(texts)\r\n",
        "    noun.extend(T)\r\n",
        "    count = Counter(noun)\r\n",
        "    noun_list = count.most_common(30)\r\n",
        "    index = 0\r\n",
        "    count_ = []\r\n",
        "    for v in noun_list:\r\n",
        "        if len(v[0]) >= 1:\r\n",
        "            index = index + 1\r\n",
        "            count_.append(v)\r\n",
        "            if index >= 3:\r\n",
        "                break\r\n",
        "\r\n",
        "    return count_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M8YuOWNNmpK"
      },
      "source": [
        "def make_data_set(ds_):#csv파일 넣기\r\n",
        "    # ds=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/result.tsv', delimiter='\\t')\r\n",
        "    ds = ds_\r\n",
        "    data = ds.iloc[:, 3]\r\n",
        "\r\n",
        "    for i in range(len(data)):\r\n",
        "        texts = data[i]\r\n",
        "        word_list = word_analysis(texts)  # 텍스트 분석해서 키워드 추출\r\n",
        "        text=\"\"\r\n",
        "        for j in word_list:\r\n",
        "          text=text+j[0]+\",\"\r\n",
        "        print(text)\r\n",
        "\r\n",
        "        ds.loc[i, 'words'] = text\r\n",
        "\r\n",
        "    ds.to_csv('/content/drive/MyDrive/Colab Notebooks/result_word.tsv', index=False, sep=\"\\t\")\r\n",
        "\r\n",
        "ds=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/result.tsv', delimiter='\\t')\r\n",
        "make_data_set(ds)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}